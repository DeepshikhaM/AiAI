{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01e1b1ac-911a-4458-9310-af2a7753b471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19722\\anaconda3\\envs\\python310\\lib\\site-packages\\torchvision\\models\\_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\19722\\anaconda3\\envs\\python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=KeypointRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=KeypointRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/keypointrcnn_resnet50_fpn_coco-fc266e95.pth\" to C:\\Users\\19722/.cache\\torch\\hub\\checkpoints\\keypointrcnn_resnet50_fpn_coco-fc266e95.pth\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 226M/226M [00:07<00:00, 33.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import KeypointRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "\n",
    "def get_keypoint_rcnn_model(num_keypoints):\n",
    "    # Load a pre-trained ResNet-FPN backbone\n",
    "    backbone = resnet_fpn_backbone('resnet50', pretrained=True)\n",
    "\n",
    "    # Define the anchor generator\n",
    "    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                       aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "    # Define the ROI pooling feature extractor\n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'],\n",
    "                                                    output_size=7,\n",
    "                                                    sampling_ratio=2)\n",
    "\n",
    "    # Define the keypoint ROI pooling feature extractor\n",
    "    keypoint_roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'],\n",
    "                                                             output_size=14,\n",
    "                                                             sampling_ratio=2)\n",
    "\n",
    "    # Define the Keypoint RCNN model\n",
    "    model = KeypointRCNN(backbone=backbone,\n",
    "                          num_classes=2,  # Including background class\n",
    "                          rpn_anchor_generator=anchor_generator,\n",
    "                          box_roi_pool=roi_pooler,\n",
    "                          keypoint_roi_pool=keypoint_roi_pooler,\n",
    "                          keypoint_head=torchvision.models.detection.keypointrcnn_resnet50_fpn(num_keypoints))\n",
    "    return model\n",
    "\n",
    "# Initialize the Keypoint RCNN model\n",
    "num_keypoints = 4  # Number of keypoints to predict\n",
    "model = get_keypoint_rcnn_model(num_keypoints)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4897a-e830-4abc-ac4a-e413485e9dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class v_0(nn.Module):\n",
    "    def __init__(self,  out_channels):\n",
    "        super(v_0, self).__init__()\n",
    "        \n",
    "        # Load EfficientNet-B7 as the backbone\n",
    "        self.backbone = EfficientNet.from_pretrained('efficientnet-b7')\n",
    "        self.backbone._fc = nn.Identity()\n",
    "        backbone_out_features = 2560\n",
    "        # Define additional layers\n",
    "        self.regression_head_1 = nn.Linear(backbone_out_features, out_channels)\n",
    "      \n",
    "        \n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Backbone feature extraction\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Apply regression heads\n",
    "        regression_output_1 = self.regression_head_1(features)\n",
    "\n",
    "        \n",
    "        return regression_output_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe7bbd07-b300-4440-a84e-c79fb13f2df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4194304])\n",
      "torch.Size([32, 8])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual  # Add residual connection\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.residual_block = ResidualBlock(in_channels, out_channels)  # Check the number of input channels here\n",
    "        self.conv = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.residual_block(x)\n",
    "        out = self.conv(out)\n",
    "        out = self.bn(out)\n",
    "        out += residual  # Add residual connection\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_regressors):\n",
    "        super(UNet, self).__init__()\n",
    "        self.down1 = DoubleConv(in_channels, 64)\n",
    "        self.down2 = DoubleConv(64, 128)\n",
    "        self.down3 = DoubleConv(128, 256)\n",
    "        self.down4 = DoubleConv(256, 512)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.up3 = DoubleConv(256 + 512, 256)\n",
    "        self.up2 = DoubleConv(128 + 256, 128)\n",
    "        self.up1 = DoubleConv(128 + 64, 64)\n",
    "\n",
    "        self.fc = nn.Linear(256 * 28 * 28, num_regressors)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.maxpool(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.maxpool(x3)\n",
    "        x5 = self.down3(x4)\n",
    "        x6 = self.maxpool(x5)\n",
    "        x7 = self.down4(x6)\n",
    "\n",
    "        x = self.upsample(x7)\n",
    "        x = torch.cat([x, x5], dim=1)\n",
    "        x = self.up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x3], dim=1)\n",
    "        x = self.up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.up1(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Reshape to (batch_size, num_regressors)\n",
    "        x = self.fc(x)  # Reshape to (batch_size, num_regressors)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "#V4 learning_rate = 0.001\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self,  out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Load EfficientNet-B7 as the backbone\n",
    "        self.backbone = EfficientNet.from_pretrained('efficientnet-b7')\n",
    "        self.backbone._fc = nn.Identity()\n",
    "        backbone_out_features = 2560\n",
    "        # Define additional layers\n",
    "        self.regression_head_1 = nn.Linear(backbone_out_features, out_channels)\n",
    "      \n",
    "        \n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Backbone feature extraction\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Apply regression heads\n",
    "        regression_output_1 = self.regression_head_1(features)\n",
    "\n",
    "        \n",
    "        return regression_output_1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# V3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_regressors):\n",
    "        super(UNet, self).__init__()\n",
    "        self.down1 = DoubleConv(in_channels, 64)\n",
    "        self.down2 = DoubleConv(64, 128)\n",
    "        self.down3 = DoubleConv(128, 256)\n",
    "        self.down4 = DoubleConv(256, 512)\n",
    "        self.down5 = DoubleConv(512, 1024)\n",
    "        self.down6 = DoubleConv(1024, 2048)  # Increase channels further\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.up5 = DoubleConv(2048 + 1024, 1024)  # Adjust channels accordingly\n",
    "        self.up4 = DoubleConv(1024 + 512, 512)\n",
    "        self.up3 = DoubleConv(512 + 256, 256)\n",
    "        self.up2 = DoubleConv(256 + 128, 128)\n",
    "        self.up1 = DoubleConv(128 + 64, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, num_regressors, kernel_size=1)\n",
    "\n",
    "        # Adjust the input size of the linear layer based on the size of the feature maps produced by the final convolutional layer\n",
    "        self.fc = nn.Linear(401408, 8)  # Assuming input image size is 256x256\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.maxpool(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.maxpool(x3)\n",
    "        x5 = self.down3(x4)\n",
    "        x6 = self.maxpool(x5)\n",
    "        x7 = self.down4(x6)\n",
    "        x8 = self.maxpool(x7)\n",
    "        x9 = self.down5(x8)\n",
    "        x10 = self.maxpool(x9)\n",
    "\n",
    "        x = self.down6(x10)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x9], dim=1)\n",
    "        x = self.up5(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x7], dim=1)\n",
    "        x = self.up4(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x5], dim=1)\n",
    "        x = self.up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x3], dim=1)\n",
    "        x = self.up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.up1(x)\n",
    "        \n",
    "        x = self.final_conv(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# V2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_regressors):\n",
    "        super(UNet, self).__init__()\n",
    "        self.down1 = DoubleConv(in_channels, 64)\n",
    "        self.down2 = DoubleConv(64, 128)\n",
    "        self.down3 = DoubleConv(128, 256)\n",
    "        self.down4 = DoubleConv(256, 512)\n",
    "        self.down5 = DoubleConv(512, 1024)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.up4 = DoubleConv(1024 + 512, 512)\n",
    "        self.up3 = DoubleConv(512 + 256, 256)\n",
    "        self.up2 = DoubleConv(256 + 128, 128)\n",
    "        self.up1 = DoubleConv(128 + 64, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, num_regressors, kernel_size=1)\n",
    "\n",
    "        self.fc = nn.Linear(401408, 8)  # Assuming input image size is 256x256\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.maxpool(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.maxpool(x3)\n",
    "        x5 = self.down3(x4)\n",
    "        x6 = self.maxpool(x5)\n",
    "        x7 = self.down4(x6)\n",
    "        x8 = self.maxpool(x7)\n",
    "        x9 = self.down5(x8)\n",
    "\n",
    "        x = self.upsample(x9)\n",
    "        x = torch.cat([x, x7], dim=1)\n",
    "        x = self.up4(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x5], dim=1)\n",
    "        x = self.up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x3], dim=1)\n",
    "        x = self.up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.up1(x)\n",
    "        \n",
    "        x = self.final_conv(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "num_channels = 3  # Input channels\n",
    "num_regressors = 8  # Number of regressors\n",
    "model = UNet(num_channels, num_regressors)\n",
    "\n",
    "# Dummy input tensor\n",
    "x = torch.randn(32, 3, 256, 256)  # Batch size of 32, 3 channels, 256x256 resolution\n",
    "output = model(x)\n",
    "print(output.size())  # Should output torch.Size([32, 8])\n",
    "'''\n",
    "\n",
    "'''\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_regressors):\n",
    "        super(UNet, self).__init__()\n",
    "        self.down1 = DoubleConv(in_channels, 64)\n",
    "        self.down2 = DoubleConv(64, 128)\n",
    "        self.down3 = DoubleConv(128, 256)\n",
    "        self.down4 = DoubleConv(256, 512)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.up3 = DoubleConv(256 + 512, 256)\n",
    "        self.up2 = DoubleConv(128 + 256, 128)\n",
    "        self.up1 = DoubleConv(128 + 64, 64)\n",
    "\n",
    "        \n",
    "        self.fc=nn.Linear(3211264,8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.maxpool(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.maxpool(x3)\n",
    "        x5 = self.down3(x4)\n",
    "        x6 = self.maxpool(x5)\n",
    "        x7 = self.down4(x6)\n",
    "\n",
    "        x = self.upsample(x7)\n",
    "        x = torch.cat([x, x5], dim=1)\n",
    "        x = self.up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x3], dim=1)\n",
    "        x = self.up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.up1(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Reshape to (batch_size, num_regressors)\n",
    "        #print(x.shape)\n",
    "        x = self.fc(x)  # Reshape to (batch_size, num_regressors)\n",
    "        return x\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2449dcd-1942-4747-a417-5c19f02a793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# V3\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Adjust the number of channels in residual connection if needed\n",
    "        self.adjust_residual = in_channels != out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x.clone().detach()  # Clone and detach to ensure the same device as input\n",
    "        #print(residual.shape)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # Adjust the residual connection if needed\n",
    "        if self.adjust_residual:\n",
    "            residual = nn.Conv2d(residual.shape[1], out.shape[1], kernel_size=1).to(x.device)(residual)\n",
    "\n",
    "        #print(out.shape)\n",
    "        out += residual  # Add residual connection\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class v_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(v_3, self).__init__()\n",
    "        \n",
    "        # Load EfficientNet-B7 as the backbone\n",
    "        self.backbone = EfficientNet.from_pretrained('efficientnet-b7')\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Replace the last three layers with nn.Identity\n",
    "        self.identity_layers()\n",
    "        \n",
    "        # Define additional layers\n",
    "        self.residual_1 = ResidualBlock(2560, 1024)\n",
    "        self.residual_2 = ResidualBlock(1024, 512)\n",
    "        self.residual_3 = ResidualBlock(512, 256)\n",
    "        self.regression_head_1 = nn.Linear(256, 8)\n",
    "        self.regression_head_2 = nn.Linear(256, 2)\n",
    "        self.regression_head_3 = nn.Linear(256, 2)\n",
    "        #print(self.backbone)\n",
    "    \n",
    "    def identity_layers(self):\n",
    "        # Replace last three layers with nn.Identity\n",
    "        \n",
    "        #self.backbone._avg_pooling = nn.Identity()\n",
    "        #self.backbone._dropout = nn.Identity()\n",
    "        self.backbone._fc = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Backbone feature extraction\n",
    "       \n",
    "        x = self.backbone(x)\n",
    "        x = x.unsqueeze(2).unsqueeze(3)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        # Pass through additional residual blocks\n",
    "        x = self.residual_1(x)\n",
    "        x = self.residual_2(x)\n",
    "        x = self.residual_3(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x_a = self.regression_head_1(x)\n",
    "        x_b = self.regression_head_2(x)\n",
    "        x_c = self.regression_head_3(x)\n",
    "        \n",
    "        return x_a, x_b, x_c\n",
    "\n",
    "\n",
    "# V2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class v_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(v_2, self).__init__()\n",
    "        self.down1 = DoubleConv(3, 64)\n",
    "        self.down2 = DoubleConv(64, 128)\n",
    "        self.down3 = DoubleConv(128, 256)\n",
    "        self.down4 = DoubleConv(256, 512)\n",
    "        self.down5 = DoubleConv(512, 1024)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.up4 = DoubleConv(1024 + 512, 512)\n",
    "        self.up3 = DoubleConv(512 + 256, 256)\n",
    "        self.up2 = DoubleConv(256 + 128, 128)\n",
    "        self.up1 = DoubleConv(128 + 64, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, 8, kernel_size=1)\n",
    "\n",
    "        #self.fc = nn.Linear(401408, 8)  # Assuming input image size is 256x256\n",
    "        self.fc1=nn.Linear(401408,8)\n",
    "        self.fc2=nn.Linear(401408,2)\n",
    "        self.fc3=nn.Linear(401408,2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.maxpool(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.maxpool(x3)\n",
    "        x5 = self.down3(x4)\n",
    "        x6 = self.maxpool(x5)\n",
    "        x7 = self.down4(x6)\n",
    "        x8 = self.maxpool(x7)\n",
    "        x9 = self.down5(x8)\n",
    "\n",
    "        x = self.upsample(x9)\n",
    "        x = torch.cat([x, x7], dim=1)\n",
    "        x = self.up4(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x5], dim=1)\n",
    "        x = self.up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x3], dim=1)\n",
    "        x = self.up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.up1(x)\n",
    "        \n",
    "        x = self.final_conv(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x_a = self.fc1(x)  # Reshape to (batch_size, num_regressors)\n",
    "        x_b = self.fc2(x)\n",
    "        x_c = self.fc3(x)\n",
    "        return x_a, x_b, x_c\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "class v_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(v_1, self).__init__()\n",
    "        self.down1 = DoubleConv(3, 64)\n",
    "        self.down2 = DoubleConv(64, 128)\n",
    "        self.down3 = DoubleConv(128, 256)\n",
    "        self.down4 = DoubleConv(256, 512)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.up3 = DoubleConv(256 + 512, 256)\n",
    "        self.up2 = DoubleConv(128 + 256, 128)\n",
    "        self.up1 = DoubleConv(128 + 64, 64)\n",
    "\n",
    "        \n",
    "        self.fc1=nn.Linear(3211264,8)\n",
    "        self.fc2=nn.Linear(3211264,2)\n",
    "        self.fc3=nn.Linear(3211264,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.maxpool(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.maxpool(x3)\n",
    "        x5 = self.down3(x4)\n",
    "        x6 = self.maxpool(x5)\n",
    "        x7 = self.down4(x6)\n",
    "\n",
    "        x = self.upsample(x7)\n",
    "        x = torch.cat([x, x5], dim=1)\n",
    "        x = self.up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x3], dim=1)\n",
    "        x = self.up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.up1(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Reshape to (batch_size, num_regressors)\n",
    "        #print(x.shape)\n",
    "        x1 = self.fc1(x)  # Reshape to (batch_size, num_regressors)\n",
    "        x2 = self.fc2(x)\n",
    "        x3 = self.fc3(x)\n",
    "        return x1, x2, x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9330f78-3f49-4fd9-9454-f86897d2b263",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dog_Hip_old(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dog_Hip_old, self).__init__()\n",
    "        \n",
    "        if pre_trained_model == 'ResNet-50':\n",
    "            self.base_model = models.resnet50(pretrained=True)\n",
    "            in_features = self.base_model.fc.in_features\n",
    "            self.base_model = nn.Sequential(*list(self.base_model.children())[:-1])\n",
    "            print('Used model: ResNet-50')\n",
    "            \n",
    "        elif pre_trained_model == 'ResNet-152':\n",
    "            self.base_model = models.resnet152(pretrained=True)\n",
    "            in_features = self.base_model.fc.in_features\n",
    "            self.base_model = nn.Sequential(*list(self.base_model.children())[:-1])\n",
    "            print('Used model: ResNet-152')\n",
    "            \n",
    "        elif pre_trained_model == 'inception_v3':\n",
    "            self.base_model = models.inception_v3(pretrained=True)\n",
    "            in_features = self.base_model.fc.in_features\n",
    "            self.base_model.fc = nn.Identity()\n",
    "            print('Used model: inception_v3')\n",
    "        \n",
    "        elif pre_trained_model == 'vgg-16':\n",
    "            self.base_model = models.vgg16(pretrained=True)\n",
    "            in_features = self.base_model.classifier[6].in_features\n",
    "            self.base_model.classifier = nn.Sequential(*list(self.base_model.classifier.children())[:-1])\n",
    "            print('Used model: VGG16')\n",
    "         \n",
    "        elif pre_trained_model == 'vgg-19':\n",
    "            self.base_model = models.vgg19(pretrained=True)\n",
    "            in_features = self.base_model.classifier[6].in_features\n",
    "            self.base_model.classifier = nn.Sequential(*list(self.base_model.classifier.children())[:-1])\n",
    "            print('Used model: VGG19')\n",
    "            \n",
    "        elif pre_trained_model == 'EfficientNet':\n",
    "            self.base_model = EfficientNet.from_pretrained('efficientnet-b7')          \n",
    "            self.base_model._fc = nn.Identity()\n",
    "            in_features = 2560\n",
    "            print('Used model: EfficientNet') \n",
    "            \n",
    "        elif pre_trained_model == 'ShuffleNet':\n",
    "            self.base_model = models.shufflenet_v2_x1_0(pretrained=True)\n",
    "            in_features = self.base_model.fc.in_features\n",
    "            self.base_model.fc = nn.Identity()\n",
    "            #in_features = 1024\n",
    "            print('Used model: ShuffleNet') \n",
    "            \n",
    "        elif pre_trained_model == 'Vit':\n",
    "            self.base_model = create_model('vit_base_patch16_224', pretrained=True)\n",
    "            self.base_model.head = nn.Identity()\n",
    "            in_features = self.base_model.num_features\n",
    "            print('Used model: Vit')\n",
    "            \n",
    "        elif pre_trained_model == 'SqueezeNet':\n",
    "            self.base_model = models.squeezenet1_0(pretrained=True)\n",
    "            self.base_model.classifier = nn.Identity()\n",
    "            in_features = 86528\n",
    "            print('Used model: SqueezeNet')\n",
    "        \n",
    "        elif pre_trained_model == 'AlexNet':\n",
    "            self.base_model = models.alexnet(pretrained=True)\n",
    "            self.base_model.classifier = nn.Identity()\n",
    "            in_features = 9216            \n",
    "            print('Used model: AlexNet')\n",
    "            \n",
    "        elif pre_trained_model == 'GoogLeNet':\n",
    "            self.base_model = models.googlenet(pretrained=True)\n",
    "            self.base_model.fc = nn.Identity()  # Remove the classification layer\n",
    "            in_features = 1024\n",
    "            print('Used model: GoogLeNet')\n",
    "        \n",
    "        elif pre_trained_model == 'MobileNetv2':\n",
    "            self.base_model = models.mobilenet_v2(pretrained=True)\n",
    "            self.base_model.classifier = nn.Identity()\n",
    "            in_features = self.base_model.last_channel\n",
    "            print('Used model: MobileNetv2')\n",
    "            \n",
    "        elif pre_trained_model == 'DenseNet161':\n",
    "            self.base_model = models.densenet161(pretrained=True)\n",
    "            self.base_model.classifier = nn.Identity()\n",
    "            in_features = 2208\n",
    "            print('Used model: DenseNet161')\n",
    "        \n",
    "        elif pre_trained_model == 'DenseNet201':\n",
    "            self.base_model = models.densenet201(pretrained=True)\n",
    "            self.base_model.classifier = nn.Identity()\n",
    "            in_features = 1920\n",
    "            print('Used model: DenseNet201')\n",
    "        \n",
    "        elif pre_trained_model == 'Xception':\n",
    "            self.base_model = create_model('xception', pretrained=True)        \n",
    "            self.base_model.last_linear = nn.Identity()\n",
    "            in_features = 1000\n",
    "            print('Used model: Xception')\n",
    "            \n",
    "        elif pre_trained_model == 'Vit_1':\n",
    "            self.base_model = create_model('vit_srelpos_medium_patch16_224', pretrained=True)\n",
    "            self.base_model.head = nn.Identity()\n",
    "            in_features = self.base_model.num_features\n",
    "            print('Used model: Vit_1')\n",
    "            \n",
    "        elif pre_trained_model == 'Vit_2':\n",
    "            self.base_model = create_model('vit_large_patch16_224', pretrained=True)\n",
    "            self.base_model.head = nn.Identity()\n",
    "            in_features = self.base_model.num_features\n",
    "            print('Used model: Vit_2')\n",
    "            \n",
    "        elif pre_trained_model == 'Vit_3':\n",
    "            self.base_model = create_model('vit_gigantic_patch14_clip_224', pretrained=True)\n",
    "            self.base_model.head = nn.Identity()\n",
    "            in_features = self.base_model.num_features\n",
    "            print('Used model: Vit_3')\n",
    "            \n",
    "        elif pre_trained_model == 'Vit_4':\n",
    "            self.base_model = create_model('vit_large_patch32_224.orig_in21k', pretrained=True, num_classes=0)\n",
    "            self.base_model.head = nn.Identity()\n",
    "            in_features = self.base_model.num_features\n",
    "            print('Used model: vit_large_patch32_224')   \n",
    "                     \n",
    "        if pre_trained_model not in ['ShuffleNet', 'inception_v3', 'EfficientNet']:\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features, 8)  # (x, y) for 4 points\n",
    "        self.fc2 = nn.Linear(in_features, 2)  # radius\n",
    "        self.fc3 = nn.Linear(in_features, 2)  # angles\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "    def forward(self, x):\n",
    "        if pre_trained_model == 'inception_v3':\n",
    "            x = self.base_model(x)\n",
    "            if isinstance(x, torchvision.models.inception.InceptionOutputs):\n",
    "            # Access the logits attribute to get the tensor from the final fully connected layer\n",
    "                x = x.logits\n",
    "            else:\n",
    "            # If the output is already a tensor, no need to change it\n",
    "                pass\n",
    "            #print('Used model: inception_v3 transform logit')\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            x = self.base_model(x)\n",
    "            \n",
    "        x = x.view(x.size(0), -1) \n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        four_points = self.fc1(x)\n",
    "        radius = self.fc2(x)\n",
    "        angles = self.fc3(x)\n",
    "        \n",
    "        return four_points, radius, angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa938b6-6380-4f78-b168-eca18aee1215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Importdata(Dataset):\n",
    "    def __init__(self, x_path, y_path, transform=True, centercrop=False, random_rotation=False):\n",
    "        self.x_path = x_path\n",
    "        self.y_path = y_path\n",
    "        self.transform = transform\n",
    "        self.centercrop = centercrop\n",
    "        self.random_rotation = random_rotation\n",
    "        \n",
    "        self.allx = sorted(os.listdir(self.x_path))\n",
    "        self.ally = sorted(os.listdir(self.y_path))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.allx)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.x_path, self.allx[idx])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        w, h = img.size        \n",
    "        \n",
    "        lab_path = os.path.join(self.y_path, self.ally[idx])\n",
    "        label = scipy.io.loadmat(lab_path)['Four_points'].astype(float)\n",
    "        label = torch.as_tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        angle = scipy.io.loadmat(lab_path)['Angles'].astype(float)\n",
    "        angle = torch.as_tensor(angle, dtype=torch.float32)\n",
    "\n",
    "        if self.transform: \n",
    "            if self.centercrop:\n",
    "                img_transform = transforms.Compose([\n",
    "                    transforms.Resize((img_size + centercrop_width, img_size + centercrop_width)),\n",
    "                    transforms.CenterCrop((img_size, img_size)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "                ])\n",
    "                \n",
    "                img = img_transform(img)\n",
    "                h_new, w_new = img.shape[1]+centercrop_width, img.shape[2]+centercrop_width\n",
    "            \n",
    "                # Rescale the locations of four points\n",
    "                label[:-1,0] = w_new/w*label[:-1,0]-(centercrop_width/2)\n",
    "                label[:-1,1] = h_new/h*label[:-1,1]-(centercrop_width/2)\n",
    "            \n",
    "            else:\n",
    "                img_transform = transforms.Compose([\n",
    "                    transforms.Resize((img_size, img_size)),\n",
    "                    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "                ])\n",
    "                \n",
    "                # Apply the transformation\n",
    "                img = img_transform(img)\n",
    "                h_new, w_new = img.shape[1], img.shape[2]\n",
    "            \n",
    "                # Rescale the locations of four points\n",
    "                label[:-1,0] = w_new/w*label[:-1,0]\n",
    "                label[:-1,1] = h_new/h*label[:-1,1]            \n",
    "            \n",
    "            # Apply random rotation\n",
    "            if self.random_rotation:\n",
    "                roangle = torch.randint(-20, 20, (1,)).item()  # Random angle between -20 and 20 degrees\n",
    "                img = torchvision.transforms.functional.rotate(img, roangle)\n",
    "                label[:-1,:] = self.rotate_points(label[:-1,:] , (img_size, img_size), roangle)\n",
    "            \n",
    "            # Radius used the mean of two re-scaled ratio\n",
    "            h_new, w_new = img.shape[1], img.shape[2]\n",
    "            label[-1,:] = torch.tensor(np.dot(label[-1,:].view(2, 1).numpy(),np.array([[w_new/w, h_new/h]])).sum(axis=1)*1/2)\n",
    "                \n",
    "        return img, label, angle, w, h, img_path\n",
    "\n",
    "    def rotate_points(self, points, image_size, angle):\n",
    "        angle = -angle  # Negative angle because torchvision rotates images clockwise\n",
    "        angle_rad = math.radians(angle)\n",
    "        center = (image_size[0] / 2, image_size[1] / 2)\n",
    "        rotated_points = []\n",
    "        for point in points:\n",
    "            x, y = point[0], point[1]\n",
    "            x -= center[0]\n",
    "            y -= center[1]\n",
    "            new_x = x * math.cos(angle_rad) - y * math.sin(angle_rad)\n",
    "            new_y = x * math.sin(angle_rad) + y * math.cos(angle_rad)\n",
    "            new_x += center[0]\n",
    "            new_y += center[1]\n",
    "            rotated_points.append([new_x, new_y])\n",
    "        return torch.tensor(rotated_points)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec49a63-57c5-4668-8684-0ef2738b5a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class v_4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(v_4, self).__init__()\n",
    "\n",
    "        self.base_model1 = EfficientNet.from_pretrained('efficientnet-b7')          \n",
    "        self.base_model1._fc = nn.Identity()\n",
    "        in_features1 = 2560\n",
    "        print('Used model: EfficientNet')\n",
    "        \n",
    "        self.fc11 = nn.Linear(in_features1, 8)  # (x, y) for 4 points\n",
    "        self.fc12 = nn.Linear(in_features1, 2)  # radius\n",
    "        self.fc13 = nn.Linear(in_features1, 2)  # angles\n",
    "\n",
    "\n",
    "        \n",
    "        self.base_model2 = create_model('vit_gigantic_patch14_clip_224', pretrained=True)\n",
    "        self.base_model2.head = nn.Identity()\n",
    "        in_features2 = self.base_model2.num_features\n",
    "\n",
    "        for param in self.base_model2.parameters():\n",
    "                param.requires_grad = False\n",
    "        print('Used model: vit_gigantic_patch14_clip_224')\n",
    "           \n",
    "               \n",
    "        self.fc21 = nn.Linear(in_features2, 8)  # (x, y) for 4 points\n",
    "        self.fc22 = nn.Linear(in_features2, 2)  # radius\n",
    "        self.fc23 = nn.Linear(in_features2, 2)  # angles\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = self.base_model1(x)\n",
    "        x1 = x1.view(x1.size(0), -1) \n",
    "\n",
    "        four_points1 = self.fc11(x1)\n",
    "        radius1 = self.fc12(x1)\n",
    "        angles1 = self.fc13(x1)\n",
    "\n",
    "        x2 = self.base_model2(x)\n",
    "        x2 = x2.view(x2.size(0), -1) \n",
    "\n",
    "        four_points2 = self.fc21(x2)\n",
    "        radius2 = self.fc22(x2)\n",
    "        angles2 = self.fc23(x2)\n",
    "\n",
    "        four_points = 1/2*(four_points1+four_points2)\n",
    "        \n",
    "        radius=1/2*(radius1+radius2)\n",
    "        \n",
    "        angles=1/2*(angles1+angles2)\n",
    "        \n",
    "        return four_points, radius, angles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae99e780-d21e-4d22-84c4-5068b636b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c214edd-982b-47ff-92a6-8720a29a4dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_def_pred_new():\n",
    "    \n",
    "    def __init__(self, loaded_model, test_loader):\n",
    "        self.model= loaded_model\n",
    "        self.test= test_loader\n",
    "        \n",
    "\n",
    "    \n",
    "    def extracted(self, all_labels):\n",
    "        p1_all, p2_all,p3_all,p4_all=[],[],[],[]\n",
    "        for i in all_labels:\n",
    "            p1, p2, p3, p4, r = i\n",
    "            p1_all.append(p1.numpy())\n",
    "            p2_all.append(p2.numpy())\n",
    "            p3_all.append(p3.numpy())\n",
    "            p4_all.append(p4.numpy())\n",
    "            #r_all.append(r.numpy())\n",
    "        return p1_all, p2_all,p3_all,p4_all\n",
    "        \n",
    "    def res(self, plot_result=None, report_loss=None):\n",
    "        c=0.\n",
    "        p_loss, a_loss, r_loss=0.,0.,0.\n",
    "        ma_p, ma_a, ma_r=0., 0., 0.\n",
    "        ma_p_ori, ma_a_ori, ma_r_ori = 0., 0., 0.\n",
    "        all_labels, all_pre_labels, all_angles, all_pre_angles, all_cal_angles = [], [], [], [],[]\n",
    "        all_files=[]\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for _, data in enumerate(self.test):\n",
    "                imgs, labels, angles, w_news, h_news, img_path = data\n",
    "                #angles = angles.to('cuda')\n",
    "                points, radius = labels[:,:-1,:].to('cuda'), labels[:,-1,:].to('cuda')\n",
    "                points = points.view(points.size(0), -1)\n",
    "                print(points.shape)\n",
    "                imgs = imgs.to('cuda')\n",
    "                pre_points = self.model(imgs)\n",
    "                print(pre_points.shape)\n",
    "                points_loss = criterion(pre_points, points.squeeze())\n",
    "\n",
    "                #angle_loss = criterion(pre_angles, angles.squeeze())\n",
    "                #radius_loss = criterion(pre_radius, radius.squeeze())\n",
    "        \n",
    "                ma_p+= MAPE(pre_points, points.squeeze()).item()\n",
    "                #ma_a+= MAPE(pre_angles, angles.squeeze()).item()\n",
    "                #ma_r+= MAPE(pre_radius, radius.squeeze()).item()\n",
    "        \n",
    "                p_loss+=points_loss.item()\n",
    "                #a_loss+=angle_loss.item()\n",
    "                #r_loss+=radius_loss.item()\n",
    "        \n",
    "                for i in range(imgs.shape[0]):\n",
    "                    #print(c)\n",
    "                    #print(img_path[i])\n",
    "                    ori_from_folder = Image.open(img_path[i]).convert('RGB')\n",
    "            \n",
    "                    img, label, angle, w_new, h_new = imgs[i].to('cpu'), labels[i], angles[i], w_news[i], h_news[i]\n",
    "                    pre_point= pre_points[i].reshape(4,2)\n",
    "            \n",
    "                    temp_r=torch.tensor([0,0]).reshape(1,2).to('cuda')\n",
    "                    pre_label = torch.cat([pre_point, temp_r], dim=0)\n",
    "           \n",
    "                    ori_img, ori_label, ori_angle = reverse_image_label(img, label, angle, w_new, h_new)\n",
    "            \n",
    "                    ori_img, orisize_pre_label, orisize_pre_angle = reverse_image_label(img, pre_label.cpu(), angle, w_new, h_new)\n",
    "\n",
    "                    #print(ori_label, orisize_pre_label,  ori_angle, orisize_pre_angle)\n",
    "            \n",
    "                    ma_p_ori+=MAPE(orisize_pre_label[:-1,:],ori_label[:-1,:]).item()\n",
    "                    #ma_r_ori+=MAPE(orisize_pre_label[-1,:],ori_label[-1,:]).item()\n",
    "                    #print(orisize_pre_angle, ori_angle)\n",
    "                    #ma_a_ori+=MAPE(orisize_pre_angle, ori_angle).item()\n",
    "\n",
    "                    angle = [round(i,2) for i in angle.tolist()[0]]\n",
    "                    #pre_angle=[round(i,2) for i in pre_angle.tolist()]\n",
    "                    #orisize_pre_angle=[round(i,2) for i in orisize_pre_angle.tolist()]\n",
    "            \n",
    "                    all_labels.append(ori_label)\n",
    "                    all_pre_labels.append(orisize_pre_label.cpu())\n",
    "                    #all_angles.append(angle)\n",
    "                    #all_pre_angles.append(pre_angle)\n",
    "                    all_cal_angles.append(orisize_pre_angle)\n",
    "                    all_files.append(img_path[i])\n",
    "                    c+=1\n",
    "                    \n",
    "                    if plot_result:    \n",
    "                        plot_images(ori_from_folder,orisize_pre_label,label_size=10)\n",
    "                        plt.title(f'{c},{angle} | cal:{orisize_pre_angle}')\n",
    "                        \n",
    "            if report_loss:\n",
    "                print(f'ave_point_loss:{round(p_loss/len(self.test),2)}')\n",
    "                print(f'mape_point_loss:{round(ma_p/len(self.test)*100,2)}')\n",
    "            \n",
    "            \n",
    "            p1_all, p2_all,p3_all,p4_all=self.extracted(all_labels)\n",
    "            print(all_pre_labels)\n",
    "            p1_all_pre, p2_all_pre,p3_all_pre,p4_all_pre= self.extracted(all_pre_labels)\n",
    "            \n",
    "            resdf = pd.DataFrame(\n",
    "                zip(all_files, p1_all, p2_all, p3_all, p4_all, all_angles, r_all,\n",
    "                    p1_all_pre, p2_all_pre, p3_all_pre, p4_all_pre, all_cal_angles, r_all_pre),\n",
    "                columns=['file','p1_all', 'p2_all', 'p3_all', 'p4_all','angle_all',\n",
    "                         'p1_all_pre', 'p2_all_pre', 'p3_all_pre', 'p4_all_pre', 'angle_all_cal']\n",
    "            )\n",
    "            resdf = resdf.explode(resdf.columns.tolist()[1:])\n",
    "        return resdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "234dd3d3-3af8-4b88-8904-ebfcd98cfd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_def_pred():\n",
    "    \n",
    "    def __init__(self, loaded_model, test_loader):\n",
    "        self.model= loaded_model\n",
    "        self.test= test_loader\n",
    "        \n",
    "\n",
    "    \n",
    "    def extracted(self, all_labels):\n",
    "        p1_all, p2_all,p3_all,p4_all,r_all=[],[],[],[],[]\n",
    "        for i in all_labels:\n",
    "            p1, p2, p3, p4, r = i\n",
    "            p1_all.append(p1.numpy())\n",
    "            p2_all.append(p2.numpy())\n",
    "            p3_all.append(p3.numpy())\n",
    "            p4_all.append(p4.numpy())\n",
    "            r_all.append(r.numpy())\n",
    "        return p1_all, p2_all,p3_all,p4_all,r_all\n",
    "        \n",
    "    def res(self, plot_result=None, report_loss=None):\n",
    "        c=0.\n",
    "        p_loss, a_loss, r_loss=0.,0.,0.\n",
    "        ma_p, ma_a, ma_r=0., 0., 0.\n",
    "        ma_p_ori, ma_a_ori, ma_r_ori = 0., 0., 0.\n",
    "        all_labels, all_pre_labels, all_angles, all_pre_angles, all_cal_angles = [], [], [], [],[]\n",
    "        all_files=[]\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for _, data in enumerate(self.test):\n",
    "                imgs, labels, angles, w_news, h_news, img_path = data\n",
    "                angles = angles.to('cuda')\n",
    "                points, radius = labels[:,:-1,:].to('cuda'), labels[:,-1,:].to('cuda')\n",
    "                points = points.view(points.size(0), -1)\n",
    "                imgs = imgs.to('cuda')\n",
    "                pre_points, pre_radius, pre_angles = self.model(imgs)\n",
    "                points_loss = criterion(pre_points, points.squeeze())\n",
    "\n",
    "                angle_loss = criterion(pre_angles, angles.squeeze())\n",
    "                radius_loss = criterion(pre_radius, radius.squeeze())\n",
    "        \n",
    "                ma_p+= MAPE(pre_points, points.squeeze()).item()\n",
    "                ma_a+= MAPE(pre_angles, angles.squeeze()).item()\n",
    "                ma_r+= MAPE(pre_radius, radius.squeeze()).item()\n",
    "        \n",
    "                p_loss+=points_loss.item()\n",
    "                a_loss+=angle_loss.item()\n",
    "                r_loss+=radius_loss.item()\n",
    "        \n",
    "                for i in range(imgs.shape[0]):\n",
    "                    #print(c)\n",
    "                    #print(img_path[i])\n",
    "                    ori_from_folder = Image.open(img_path[i]).convert('RGB')\n",
    "            \n",
    "                    img, label, angle, w_new, h_new = imgs[i].to('cpu'), labels[i], angles[i], w_news[i], h_news[i]\n",
    "                    pre_point, pre_radiu, pre_angle = pre_points[i].reshape(4,2), pre_radius[i].reshape(1,2), pre_angles[i]\n",
    "            \n",
    "                    pre_label = torch.cat([pre_point, pre_radiu], dim=0)\n",
    "           \n",
    "                    ori_img, ori_label, ori_angle = reverse_image_label(img, label, angle, w_new, h_new)\n",
    "            \n",
    "                    ori_img, orisize_pre_label, orisize_pre_angle = reverse_image_label(img, pre_label.cpu(), pre_angle.cpu(), w_new, h_new)\n",
    "\n",
    "                    #print(ori_label, orisize_pre_label,  ori_angle, orisize_pre_angle)\n",
    "            \n",
    "                    ma_p_ori+=MAPE(orisize_pre_label[:-1,:],ori_label[:-1,:]).item()\n",
    "                    ma_r_ori+=MAPE(orisize_pre_label[-1,:],ori_label[-1,:]).item()\n",
    "                    #print(orisize_pre_angle, ori_angle)\n",
    "                    ma_a_ori+=MAPE(orisize_pre_angle, ori_angle).item()\n",
    "\n",
    "                    angle = [round(i,2) for i in angle.tolist()[0]]\n",
    "                    pre_angle=[round(i,2) for i in pre_angle.tolist()]\n",
    "                    orisize_pre_angle=[round(i,2) for i in orisize_pre_angle.tolist()]\n",
    "            \n",
    "                    all_labels.append(ori_label)\n",
    "                    all_pre_labels.append(orisize_pre_label.cpu())\n",
    "                    all_angles.append(angle)\n",
    "                    all_pre_angles.append(pre_angle)\n",
    "                    all_cal_angles.append(orisize_pre_angle)\n",
    "                    all_files.append(img_path[i])\n",
    "                    c+=1\n",
    "                    \n",
    "                    if plot_result:    \n",
    "                        plot_images(ori_from_folder,orisize_pre_label,label_size=10)\n",
    "                        plt.title(f'{c},{angle} | pre:{pre_angle}, cal:{orisize_pre_angle}')\n",
    "                        \n",
    "            if report_loss:\n",
    "                print(f'ave_point_loss:{round(p_loss/len(self.test),2)}, ave_angle_loss:{round(a_loss/len(self.test),2)},ave_radi_loss:{round(r_loss/len(self.test),2)}')\n",
    "                print(f'mape_point_loss:{round(ma_p/len(self.test)*100,2)}, mape_angle_loss:{round(ma_a/len(self.test)*100,2)},mape_radi_loss:{round(ma_r/len(self.test)*100,2)}')\n",
    "            \n",
    "            \n",
    "            p1_all, p2_all,p3_all,p4_all,r_all=self.extracted(all_labels)\n",
    "            p1_all_pre, p2_all_pre,p3_all_pre,p4_all_pre,r_all_pre = self.extracted(all_pre_labels)\n",
    "            \n",
    "            resdf = pd.DataFrame(\n",
    "                zip(all_files, p1_all, p2_all, p3_all, p4_all, r_all, all_angles,\n",
    "                    p1_all_pre, p2_all_pre, p3_all_pre, p4_all_pre, r_all_pre, all_pre_angles, all_cal_angles),\n",
    "                columns=['file','p1_all', 'p2_all', 'p3_all', 'p4_all', 'r_all', 'angle_all',\n",
    "                         'p1_all_pre', 'p2_all_pre', 'p3_all_pre', 'p4_all_pre', 'r_all_pre', 'angle_all_pre', 'angle_all_cal']\n",
    "            )\n",
    "            resdf = resdf.explode(resdf.columns.tolist()[1:])\n",
    "        return resdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bb1c87e-ef95-4268-88fc-150b008603e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_image_label(img, label, angles, w_new, h_new): # w_new, h_new are the original size of image\n",
    "    _, w, h = img.shape\n",
    "    \n",
    "    label = label.clone()\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])\n",
    "    # Undo the normalization\n",
    "    img = img * std[:, None, None] + mean[:, None, None]\n",
    "    img_pil = transforms.ToPILImage()(img)\n",
    "    \n",
    "    if centercrop_p:\n",
    "        left_padding = int(centercrop_width/2)\n",
    "        top_padding = int(centercrop_width/2)\n",
    "        right_padding = int(centercrop_width/2)\n",
    "        bottom_padding = int(centercrop_width/2)\n",
    "        img_pil = ImageOps.expand(img_pil, (left_padding, top_padding, right_padding, bottom_padding), fill=(255, 255, 255))\n",
    "        # Rescale the locations of four points\n",
    "        w, h =w+centercrop_width, h+centercrop_width\n",
    "        label[:-1,0] = w_new/w*(label[:-1,0]+(centercrop_width/2))\n",
    "        label[:-1,1] = h_new/h*(label[:-1,1]+(centercrop_width/2))\n",
    "    else:\n",
    "        # Rescale the locations of four points\n",
    "        label[:-1,0] = w_new/w*label[:-1,0]\n",
    "        label[:-1,1] = h_new/h*label[:-1,1]\n",
    "    \n",
    "    resize_transform = transforms.Resize((h_new, w_new))  # Use the original image size here\n",
    "    img_original_size = resize_transform(img_pil)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Radius reverse it back to original radius\n",
    "    label[-1,:] = label[-1,:]*2/(w/w_new + h/h_new)\n",
    "    \n",
    "    # Rescale the angle (used the four points to calculate angles)\n",
    "    p1,p2,p3,p4,r = label\n",
    "    p_12 = p2-p1\n",
    "    p_13 = p3-p1\n",
    "    p_24 = p4-p2\n",
    "    p_21 = p1-p2\n",
    "    cosine_angle_1 = np.dot(p_12, p_13) / (np.linalg.norm(p_12) * np.linalg.norm(p_13))\n",
    "    cosine_angle_2 = np.dot(p_24, p_21) / (np.linalg.norm(p_24) * np.linalg.norm(p_21))\n",
    "    angle_1 = np.arccos(cosine_angle_1)\n",
    "    angle_2 = np.arccos(cosine_angle_2)\n",
    "    angle_1 = np.degrees(angle_1)\n",
    "    angle_2 = np.degrees(angle_2)\n",
    "    angle = torch.as_tensor(np.array([angle_1,angle_2]), dtype=torch.float32) \n",
    "    \n",
    "    return img_original_size, label, angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb32b399-790e-4b8f-8d52-dca4552cca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_square(all_angles,all_cal_angles):\n",
    "    '''\n",
    "    all_angles,all_cal_angles are series\n",
    "    \n",
    "    '''\n",
    "    y = np.array(all_angles.tolist())\n",
    "    mean_y = y.mean(axis=0)\n",
    "    y_hat =  np.array(all_cal_angles.tolist())\n",
    "    TSS = np.square(y-mean_y).sum(axis=0)\n",
    "    RSS = np.square(y-y_hat).sum(axis=0)\n",
    "    #print('TSS',TSS)\n",
    "    #print('RSS',RSS)\n",
    "    return(round(1-(RSS/TSS),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec5933c0-e16e-4021-a7a6-c1576eca5c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(pre_y, y):\n",
    "    #print(y-pre_y,'..........')\n",
    "    #print(y)\n",
    "    #print(pre_y)\n",
    "    \n",
    "    temp= torch.abs(y-pre_y)/torch.abs(y)\n",
    "    return torch.mean(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaa63f8-04b4-4d53-a228-cfb140634847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(img,label, label_size=1):\n",
    "\n",
    "    p1, p2, p3, p4, rs = label\n",
    "    \n",
    "    circle1 = plt.Circle((p1[0], p1[1]), rs[0], color='r', fill=False)\n",
    "    circle2 = plt.Circle((p2[0], p2[1]), rs[1], color='r', fill=False)\n",
    "    p_1 = plt.Circle((p1[0], p1[1]), label_size, color='g', fill=False)\n",
    "    p_2 = plt.Circle((p2[0], p2[1]), label_size, color='r', fill=False)\n",
    "    p_3 = plt.Circle((p3[0], p3[1]), label_size, color='b', fill=True)\n",
    "    p_4 = plt.Circle((p4[0], p4[1]), label_size, color='y', fill=True)\n",
    "    \n",
    "    if isinstance(img, torch.Tensor):\n",
    "        image = np.transpose(img, (1, 2, 0))\n",
    "    else: image = img\n",
    "\n",
    "    fig,ax = plt.subplots(figsize=(10,10))\n",
    "    ax.set_aspect('equal')\n",
    "    plt.plot([p1[0], p2[0]], [p1[1], p2[1]], color=\"g\", linewidth=1)\n",
    "    plt.plot([p1[0], p3[0]], [p1[1], p3[1]], color=\"g\", linewidth=1)\n",
    "    plt.plot([p2[0], p4[0]], [p2[1], p4[1]], color=\"g\", linewidth=1)\n",
    "    \n",
    "\n",
    "    # Show the image\n",
    "    ax.imshow(image)\n",
    "    ax.add_patch(circle1)\n",
    "    ax.add_patch(circle2)\n",
    "    ax.add_patch(p_1)\n",
    "    ax.add_patch(p_2)\n",
    "    ax.add_patch(p_3)\n",
    "    ax.add_patch(p_4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
